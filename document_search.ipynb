{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjJVCU6bvGns5adWb1F19K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thedatadj/natural-language-processing/blob/main/document_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "nn3TyFSzES64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "nltk.download('twitter_samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w0Mup2BEdox",
        "outputId": "45bd8c97-79e7-4d85-8bbe-bdf96552fdb7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load set of positive tweets and negative tweets stored in a list."
      ],
      "metadata": {
        "id": "kGgglBbiE-fu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6GP1LtHBD4bP"
      },
      "outputs": [],
      "source": [
        "# Positive\n",
        "posset = twitter_samples.strings('positive_tweets.json')\n",
        "\n",
        "# Negative\n",
        "negset = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# All tweets\n",
        "fullset = posset + negset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings\n",
        "Load and embedding dictionary where the key is the word, and the value is the embedding vector of that word.\n",
        "* Each embedding is a 300-dimensional vector."
      ],
      "metadata": {
        "id": "f3BVDTRNFo_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download file\n",
        "!gdown 16TjH95jhGzqA8f0zSf0uOrAYM_E0aYtT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCUa_ytHFZCa",
        "outputId": "6a71a413-8bfd-4924-ab65-eeed7588d761"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16TjH95jhGzqA8f0zSf0uOrAYM_E0aYtT\n",
            "To: /content/en_embeddings.p\n",
            "\r  0% 0.00/8.12M [00:00<?, ?B/s]\r 58% 4.72M/8.12M [00:00<00:00, 31.1MB/s]\r100% 8.12M/8.12M [00:00<00:00, 49.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load file\n",
        "import pickle\n",
        "en_embeddings_subset = pickle.load(open(\"/content/en_embeddings.p\", \"rb\"))"
      ],
      "metadata": {
        "id": "I2_GthNeF3C6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First 10 components of the embedding vector of the word \"the\"."
      ],
      "metadata": {
        "id": "YUNgDf7VF645"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_embeddings_subset['the'][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISRUs-67F5eS",
        "outputId": "a489a863-6e23-4334-bb57-7106f206556d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.08007812,  0.10498047,  0.04980469,  0.0534668 , -0.06738281,\n",
              "       -0.12060547,  0.03515625, -0.11865234,  0.04394531,  0.03015137],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tweet to embedding\n",
        "Function that transform a tweet into a vector representation of that tweet.\n",
        "* Using the vector representation of each word in the tweet."
      ],
      "metadata": {
        "id": "Uk-NzXG5GYVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess the tweet\n",
        "!gdown 1vi5cAPha1-n1OKx9ke_xhkoVQQYE2hDu\n",
        "from utils import process_tweet\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tk-JLddHmGH",
        "outputId": "20bd1363-5006-4eef-fec0-711877f172d0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vi5cAPha1-n1OKx9ke_xhkoVQQYE2hDu\n",
            "To: /content/utils.py\n",
            "\r  0% 0.00/2.67k [00:00<?, ?B/s]\r100% 2.67k/2.67k [00:00<00:00, 9.23MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Q0vccYnOIBPw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tweetembedding(tweet, embeddings):\n",
        "    # Initialize the tweet embedding\n",
        "    tweetembedding0 = np.zeros(300)\n",
        "\n",
        "    # Process the tweet\n",
        "    tokens = process_tweet(tweet)\n",
        "\n",
        "    # Accumulate embeddings values\n",
        "    for token in tokens:\n",
        "        tweetembedding0 += embeddings.get(token, 0)\n",
        "\n",
        "    return tweetembedding0"
      ],
      "metadata": {
        "id": "Ivsn93AdGAqr"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demostration\n",
        "tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
        "embedding0 = tweetembedding(tweet, en_embeddings_subset)\n",
        "embedding0[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r52GWzssIYxM",
        "outputId": "63f637e0-bb34-4d48-a55d-6411fba0f8c1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.09228516,  0.35986328, -0.0206604 ,  0.63085938, -0.06640625])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding dataset\n",
        "Get a dictionary and a numpy array containing the embeddings of all the tweets in the `fullset` dataset."
      ],
      "metadata": {
        "id": "rnd1-0eSJwvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddingdic = {}\n",
        "embeddingset = []\n",
        "for i, tweet in enumerate(fullset):\n",
        "    embedding = tweetembedding(tweet, en_embeddings_subset)\n",
        "    embeddingdic[i] = embedding\n",
        "    embeddingset.append(embedding)\n",
        "\n",
        "embeddingset = np.array(embeddingset)"
      ],
      "metadata": {
        "id": "2AqHNyJbIoYU"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similar tweet\n",
        "Given a tweet, search another similar tweet in the dataset."
      ],
      "metadata": {
        "id": "SySdeF2MMqIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet1 = 'i am sad'"
      ],
      "metadata": {
        "id": "i58FlxuOMbxr"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import cosine_similarity"
      ],
      "metadata": {
        "id": "zzoeLAJeNO-S"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector representation of the tweet\n",
        "embedding1 = tweetembedding(tweet1, en_embeddings_subset)\n",
        "\n",
        "# Get similarity with each tweet in the dataset\n",
        "similarities = cosine_similarity(embeddingset, embedding1)"
      ],
      "metadata": {
        "id": "LmrkDXCuMzri"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(similarities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scBC8FeENd-c",
        "outputId": "cfa61f7c-c142-4eee-d5f2-cce2bd3ee876"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5202"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fullset[5202]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "28KzGQAiNhUS",
        "outputId": "70e9e76d-aeef-46cd-bfda-713f5d09fa57"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'@hanbined sad pray for me :((('"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similar tweet with LSH\n",
        "Same as before but using locality sensitive hashing."
      ],
      "metadata": {
        "id": "sMq9LKzkONXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of planes\n",
        "n = 10\n",
        "\n",
        "# Iterations\n",
        "iterations = 25"
      ],
      "metadata": {
        "id": "q8uCsga9NzUx"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random planes\n",
        "Create a set containing:\n",
        "* 25 set of 10 planes."
      ],
      "metadata": {
        "id": "ZcPJFa0XQFt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "m = 300\n",
        "def tenplanes():\n",
        "    planes = np.random.normal(size=(m, n))\n",
        "    return planes\n",
        "\n",
        "planessets = [tenplanes() for i in range(iterations)]"
      ],
      "metadata": {
        "id": "Vvzfcp-wQOfn"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hash function\n",
        "Function that:\n",
        "* Outputs the hash value of a vector."
      ],
      "metadata": {
        "id": "S4NMhilEP1el"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hashvalue(vector, planes):\n",
        "    dot = vector.dot(planes)\n",
        "    signs = np.sign(dot)\n",
        "    localhash = (signs >= 0).astype(int)\n",
        "    localhash = localhash.flatten()\n",
        "\n",
        "    globalhash = 0\n",
        "\n",
        "    for i in range(n):\n",
        "        globalhash += 2**i * localhash[i]\n",
        "\n",
        "    globalhash = int(globalhash)\n",
        "    return globalhash"
      ],
      "metadata": {
        "id": "uOr410gxP8B8"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hash table"
      ],
      "metadata": {
        "id": "X48UePmoTCpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hashtablem(vectors, planes):\n",
        "    # Number of planes\n",
        "    n = 10\n",
        "\n",
        "    # Number of hash values\n",
        "    nh = 2**n\n",
        "\n",
        "    # Initialize hash table and id table\n",
        "    hashtable = {}\n",
        "    idtable = {}\n",
        "    for i in range(nh):\n",
        "        hashtable[i] = []\n",
        "        idtable[i] = []\n",
        "\n",
        "    for i, vector in enumerate(embeddingset):\n",
        "        ihashvalue = hashvalue(vector, planes)\n",
        "        hashtable[ihashvalue].append(vector)\n",
        "        idtable[ihashvalue].append(i)\n",
        "\n",
        "    return hashtable, idtable"
      ],
      "metadata": {
        "id": "GOizA7_FTAOa"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hash table per iteration\n",
        "Create a hash table per each iteration."
      ],
      "metadata": {
        "id": "Y0ArTwX7VxaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hashtable25():\n",
        "    hashtables = []\n",
        "    idtables = []\n",
        "    for i in range(iterations):\n",
        "        planes = planessets[i]\n",
        "        hashtable, idtable = hashtablem(embeddingset, planes)\n",
        "        hashtables.append(hashtable)\n",
        "        idtables.append(idtable)\n",
        "    return hashtables, idtables\n",
        "\n",
        "hashtables, idtables = hashtable25()"
      ],
      "metadata": {
        "id": "KFb47hpOVw0l"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K nearest neighbors"
      ],
      "metadata": {
        "id": "rFnRFo_FWtWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nearest_neighbor(v, candidates, k=1, cosine_similarity=cosine_similarity):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - v, the vector you are going find the nearest neighbor for\n",
        "      - candidates: a set of vectors where we will find the neighbors\n",
        "      - k: top k nearest neighbors to find\n",
        "    Output:\n",
        "      - k_idx: the indices of the top k closest vectors in sorted form\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    similarity_l = []\n",
        "\n",
        "    # for each candidate vector...\n",
        "    for row in candidates:\n",
        "        # get the cosine similarity\n",
        "        cos_similarity = cosine_similarity(v, row)\n",
        "\n",
        "        # append the similarity to the list\n",
        "        similarity_l.append(cos_similarity)\n",
        "\n",
        "    # sort the similarity list and get the indices of the sorted list\n",
        "    sorted_ids = np.argsort(similarity_l)\n",
        "\n",
        "    # Reverse the order of the sorted_ids array\n",
        "    sorted_ids = np.flip(sorted_ids)\n",
        "\n",
        "    # get the indices of the k most similar candidate vectors\n",
        "    k_idx = sorted_ids[:k]\n",
        "    ### END CODE HERE ###\n",
        "    return k_idx"
      ],
      "metadata": {
        "id": "Ux7mCbkAaC3t"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def knn(index, vector, planeset, hashtables, idtables, k=1, iterations=25):\n",
        "    candidates = []\n",
        "    ids = []\n",
        "    ids_canditates = set()\n",
        "\n",
        "    for i in range(iterations):\n",
        "        planes = planeset[i]\n",
        "        ihashvalue = hashvalue(vector, planes)\n",
        "        hashtable = hashtables[i]\n",
        "        embeddings = hashtable[ihashvalue]\n",
        "        idtable = idtables[i]\n",
        "        newidcandidates = idtable[ihashvalue]\n",
        "\n",
        "        for i, new_id in enumerate(newidcandidates):\n",
        "            if index == new_id:\n",
        "                continue\n",
        "\n",
        "            if new_id not in ids_canditates:\n",
        "                ithembedding = embeddings[i]\n",
        "                candidates.append(ithembedding)\n",
        "                ids.append(new_id)\n",
        "                ids_canditates.add(new_id)\n",
        "\n",
        "    vectors_candidate = np.array(candidates)\n",
        "    nearest = nearest_neighbor(vector, vectors_candidate, k=k)\n",
        "    nnids = [ids[idx] for idx in nearest]\n",
        "\n",
        "    return nnids"
      ],
      "metadata": {
        "id": "kWY7DQAxWohf"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_id = 0\n",
        "doc_to_search = fullset[doc_id]\n",
        "vec_to_search = embeddingset[doc_id]"
      ],
      "metadata": {
        "id": "bDIc3vaHahNx"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_neighbor_ids = knn(\n",
        "    doc_id, vec_to_search, planessets, hashtables, idtables, k=3, iterations=5)"
      ],
      "metadata": {
        "id": "tpFYA55wapML"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_neighbor_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOEHAz3oaq5Y",
        "outputId": "b3d78512-d2a1-4285-bb1a-f8e97de3fc80"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[51, 2478, 105]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Nearest neighbors for document {doc_id}\")\n",
        "print(f\"Document contents: {doc_to_search}\")\n",
        "print(\"\")\n",
        "\n",
        "for neighbor_id in nearest_neighbor_ids:\n",
        "    print(f\"Nearest neighbor at document id {neighbor_id}\")\n",
        "    print(f\"document contents: {fullset[neighbor_id]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5Uql6KlbJAR",
        "outputId": "7ff0b9b8-cd61-4b53-d14c-d0ffa872dd56"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest neighbors for document 0\n",
            "Document contents: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "Nearest neighbor at document id 51\n",
            "document contents: #FollowFriday @France_Espana @reglisse_menthe @CCI_inter for being top engaged members in my community this week :)\n",
            "Nearest neighbor at document id 2478\n",
            "document contents: #ShareTheLove @oymgroup @musicartisthere for being top HighValue members this week :) @nataliavas http://t.co/IWSDMtcayt\n",
            "Nearest neighbor at document id 105\n",
            "document contents: #FollowFriday @straz_das @DCarsonCPA @GH813600 for being top engaged members in my community this week :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3iFS7CfBbKjK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}