{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxIKBFrxco/hnHaIi3Q5W/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thedatadj/natural-language-processing/blob/main/autocomplete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autocomplete model that given the initial words of a sentence it suggest the rest of the sentence."
      ],
      "metadata": {
        "id": "xwJfuW6lRZDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data\n",
        "The data consist of a set of tweet."
      ],
      "metadata": {
        "id": "sLDU0ZhJSaRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download file\n",
        "!gdown 13Qzds0_08hG0JNppbro1PElQzmi0hJ96"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UljPT-UTB8x",
        "outputId": "b5653c91-2562-4065-9a7d-cc07282c37c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13Qzds0_08hG0JNppbro1PElQzmi0hJ96\n",
            "To: /content/en_US.twitter.txt\n",
            "\r  0% 0.00/3.39M [00:00<?, ?B/s]\r100% 3.39M/3.39M [00:00<00:00, 45.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data file\n",
        "with open(\"/content/en_US.twitter.txt\", \"r\") as file:\n",
        "    data = file.read()\n",
        "data[:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Hf4mHBzMTGF9",
        "outputId": "2b51a011-d3e6-445c-abaa-bee3d2898086"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each tweet is separated by the special character `\\n`."
      ],
      "metadata": {
        "id": "OiHzH2h6Ti7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess data"
      ],
      "metadata": {
        "id": "LnOsB5gzScn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`data` is a continuous string, I split this string into individual sentences/tweets."
      ],
      "metadata": {
        "id": "CxlN6Xt_T8uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data0 = [tweet for tweet in data.split(\"\\n\")]\n",
        "data0[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNNnCHUvUOwb",
        "outputId": "bd8c45d6-886a-444e-e207-82338aa89b33"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.',\n",
              " \"When you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There could be sentences/tweets that start or end with a space. I remove these spaces."
      ],
      "metadata": {
        "id": "r3v0oFqrUlWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = [tweet.strip() for tweet in data0]"
      ],
      "metadata": {
        "id": "aoZXGTluUvlV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There could be tweets that are empty string, these don't add information to the model so I eliminate them."
      ],
      "metadata": {
        "id": "wvVQG8fOVY5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = [tweet for tweet in data1 if len(tweet) > 0]"
      ],
      "metadata": {
        "id": "yuv--68TVYii"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize each sentence."
      ],
      "metadata": {
        "id": "SAVVFBPoXO0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data3 = [tweet.lower() for tweet in data2]\n",
        "data3[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iO_0Ast-XSje",
        "outputId": "84763961-2169-487c-da3c-d6d1893e9557"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'how are you? btw thanks for the rt. you gonna be in dc anytime soon? love to see you. been way, way too long.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I tokenize each sentence in the dataset."
      ],
      "metadata": {
        "id": "-GW19EgbWGAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool for tokenization\n",
        "import re"
      ],
      "metadata": {
        "id": "eQfBSWuCWsmw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data4 = []\n",
        "for tweet in data3:\n",
        "    tweet_tk = re.findall(r\"\\w+|\\.|\\,\", tweet)\n",
        "    data4.append(tweet_tk)\n",
        "data4[0][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-tqpNWMWPyR",
        "outputId": "7dbaa84d-1ec2-4017-bbe9-fc8608c0cdd3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['how', 'are', 'you', 'btw', 'thanks', 'for', 'the', 'rt', '.', 'you']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into training and testing sets."
      ],
      "metadata": {
        "id": "k3hA78ngYlpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For randomized assigning\n",
        "import random"
      ],
      "metadata": {
        "id": "oDi8atb3YtcC"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(87)\n",
        "\n",
        "# Shuffle the data\n",
        "random.shuffle(data4)"
      ],
      "metadata": {
        "id": "DxgXoFwrYrKk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assign 80% of the data for training."
      ],
      "metadata": {
        "id": "Fnnlx-YbZFRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_size = int(len(data4) * 0.8)\n",
        "train_data = data4[:split_size]"
      ],
      "metadata": {
        "id": "CHztXEJ_Y9Aq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assign the rest 20% for testing."
      ],
      "metadata": {
        "id": "MaBYQWKgZY2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = data4[split_size:]"
      ],
      "metadata": {
        "id": "Gb6HatTLZYdR"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I create a dictionary of frequencies of words from the training data."
      ],
      "metadata": {
        "id": "YKvOmXF9ZxRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_train = {}\n",
        "for sentence in train_data:\n",
        "    for token in sentence:\n",
        "        if token not in freq_train:\n",
        "            freq_train[token] = 0\n",
        "        freq_train[token] += 1"
      ],
      "metadata": {
        "id": "adgqbmZMaBMJ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(freq_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikE3oR8U8hwd",
        "outputId": "45015379-6c0a-48d3-8563-ffdefe41437d"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32537"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency of \"the\" in training set\n",
        "freq_train['the']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAF8tpscacsn",
        "outputId": "db8517b4-935b-497b-b267-7db2e9d6526e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15314"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There might be words in the testing set that are not in the training set. I consider low frequency words in the training set as out of vocabulary words.\n",
        "* Create a vocabulary of frequent words.\n",
        "* Considere all other words as unknown (oov)"
      ],
      "metadata": {
        "id": "GS_Ij2ytavwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimum frequency\n",
        "minfreq = 2"
      ],
      "metadata": {
        "id": "asjcwQ98bfo9"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = []\n",
        "for word, freq in freq_train.items():\n",
        "    if freq >= minfreq:\n",
        "        vocab.append(word)"
      ],
      "metadata": {
        "id": "oX6nJwWPbYm-"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-06BJA9X7rSS",
        "outputId": "509325ae-4750-4669-b3ad-998a1b504734"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14585"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAL6EniPcA_k",
        "outputId": "7bd52c35-4e80-428b-8078-ec812217c20c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'personally', 'would', 'like', 'as']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace low frequency words in `train_data` for the special token\n",
        "* `<ukn>`"
      ],
      "metadata": {
        "id": "K79UHUHncGCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_temp = []\n",
        "for sentence in train_data:\n",
        "    sentence_range = []\n",
        "    for token in sentence:\n",
        "        if token not in vocab:\n",
        "            token = \"<unk>\"\n",
        "        sentence_range.append(token)\n",
        "    train_data_temp.append(sentence_range)\n",
        "train_data1 = train_data_temp"
      ],
      "metadata": {
        "id": "Tn4SDiPIcbAe"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data1[5][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrZ33H0Odh_f",
        "outputId": "bebd3058-6838-47c8-8cbf-a5c7c05bd442"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['baby', 'j', 'at', 'jordan', 'ford', '<unk>', '<unk>', '35', 'n', 'w']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the previous operation on the test dataset."
      ],
      "metadata": {
        "id": "cV_bqPFtediI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_temp = []\n",
        "for sentence in test_data:\n",
        "    sentence_range = []\n",
        "    for token in sentence:\n",
        "        if token not in vocab:\n",
        "            token = \"<unk>\"\n",
        "        sentence_range.append(token)\n",
        "    test_data_temp.append(sentence_range)\n",
        "test_data1 = test_data_temp"
      ],
      "metadata": {
        "id": "SQ4QrEJke4aH"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data1[5][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9nPCy5vfBC9",
        "outputId": "8d587f7c-aa69-41cc-ffdb-6d760251303f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['full', 'house', 'for', '<unk>', 'of', '<unk>']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-gram language model"
      ],
      "metadata": {
        "id": "zYeqsTqISgHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function that computes the counts of n-grams for an arbitraty number n.\n",
        "* Output: dictionary."
      ],
      "metadata": {
        "id": "iyIFZUC7hiLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = ['I', \"like\", \"rice\"]\n",
        "sentence = tuple(sentence)\n",
        "for i in range(len(sentence)):\n",
        "    ngram = sentence[i:i+2]\n",
        "    print(ngram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PdvJbMRi9TD",
        "outputId": "94c783d6-1a69-4007-a5f5-20d829bcf8f5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I', 'like')\n",
            "('like', 'rice')\n",
            "('rice',)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_ngrams(data, n, start_token=\"<s>\", end_token=\"<e>\"):\n",
        "    ngrams = {}\n",
        "    for sentence in data:\n",
        "        # Add special tokens\n",
        "        sentence = [start_token] * n + sentence + [end_token]\n",
        "        sentence = tuple(sentence)\n",
        "        for i in range(len(sentence)-1):\n",
        "            ngram = sentence[i:i+n]\n",
        "            if ngram in ngrams:\n",
        "                ngrams[ngram] += 1\n",
        "            else:\n",
        "                ngrams[ngram] = 1\n",
        "    return ngrams"
      ],
      "metadata": {
        "id": "Kaph6wAdiHxB"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that estimates the probability of a word given a sequence sequence of n words."
      ],
      "metadata": {
        "id": "NpmhUR3Ulzv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_probability(word, previous_n_gram, n_gram_counts,\n",
        "                         n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
        "    denominator = previous_n_gram_count + k * vocabulary_size\n",
        "    n_plus1_gram = previous_n_gram + (word,)\n",
        "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)\n",
        "    numerator = n_plus1_gram_count + k\n",
        "    probability = numerator / denominator\n",
        "    return probability"
      ],
      "metadata": {
        "id": "N6UpGB6nmdMg"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estimate the probability for all words in training set."
      ],
      "metadata": {
        "id": "y8bo-GD-oANa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_probabilities(previous_n_gram, n_gram_counts,\n",
        "                           n_plus1_gram_counts, vocabulary,\n",
        "                           end_token=\"<e>\", unknown_token=\"<unk>\",\n",
        "                           k=1.0):\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "    # Add special characters to the vocabulary\n",
        "    vocabulary = vocabulary + [end_token, unknown_token]\n",
        "    vocabulary_size = len(vocabulary)\n",
        "\n",
        "    probabilities = {}\n",
        "    for word in vocabulary:\n",
        "        probability = estimate_probability(word, previous_n_gram,\n",
        "                                           n_gram_counts, n_plus1_gram_counts,\n",
        "                                           vocabulary_size, k=k)\n",
        "        probabilities[word] = probability\n",
        "    return probabilities"
      ],
      "metadata": {
        "id": "x8G-6d8boGg3"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a probability matrix"
      ],
      "metadata": {
        "id": "QrqQdMc_pRNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tools for creating the matrix\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "HxkHO9e-qjXr"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_cmatrix(n_plus1_gram_counts, vocabulary):\n",
        "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
        "    ngrams = []\n",
        "    for n_plus1_gram in n_plus1_gram_counts:\n",
        "        ngram = n_plus1_gram[0:-1]\n",
        "        ngrams.append(ngram)\n",
        "    ngrams = list(set(ngrams))\n",
        "    # N-gram to row\n",
        "    row_index = {ngram:i for i, ngram in enumerate(ngrams)}\n",
        "    # N-gram to column\n",
        "    col_index = {word:j for j, word in enumerate(vocabulary)}\n",
        "\n",
        "    nrow = len(ngrams)\n",
        "    ncol = len(vocabulary)\n",
        "    cmatrix = np.zeros((nrow, ncol))\n",
        "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
        "        ngram = n_plus1_gram[0: -1]\n",
        "        word = n_plus1_gram[-1]\n",
        "        if word not in vocabulary:\n",
        "            continue\n",
        "        i = row_index[ngram]\n",
        "        j = col_index[word]\n",
        "        cmatrix[i, j] = count\n",
        "\n",
        "    cmatrix = pd.DataFrame(cmatrix, index=ngrams, columns=vocabulary)\n",
        "    return cmatrix"
      ],
      "metadata": {
        "id": "mTeXZpWTpkYV"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_pmatrix(n_plus1_gram_counts, vocabulary, k):\n",
        "    cmatrix = make_cmatrix(n_plus1_gram_counts, vocabulary)\n",
        "    cmatrix = cmatrix + k\n",
        "    pmatrix = cmatrix.div(cmatrix.sum(axis=1), axis=0)\n",
        "    return pmatrix"
      ],
      "metadata": {
        "id": "Rqj688rnr3Qi"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a sequence of words, suggest the most likely next word."
      ],
      "metadata": {
        "id": "35sFpnZS1K29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts,\n",
        "                   vocabulary, end_token=\"<e>\", unknown_token=\"<s>\",\n",
        "                   k=1.0, start_with=None):\n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "    previous_tokens = ['<s>'] * n + previous_tokens\n",
        "    previous_n_gram = previous_tokens[-n]\n",
        "    probabilities = estimate_probabilities(previous_n_gram, n_gram_counts,\n",
        "                                           n_plus1_gram_counts,\n",
        "                                           vocabulary, k=k)\n",
        "    suggestion = None\n",
        "    max_prob = 0\n",
        "    for word, prob in probabilities.items():\n",
        "        if start_with:\n",
        "            if word[0] != start_with[0]:\n",
        "                continue\n",
        "        if max_prob < prob:\n",
        "            suggestion = word\n",
        "            max_prob = prob\n",
        "    return suggestion, max_prob"
      ],
      "metadata": {
        "id": "HDddl_rvy1pI"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suggest multiple next words."
      ],
      "metadata": {
        "id": "EeojF6P51H2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_suggestions(previous_tokens, n_gram_counts_list,\n",
        "                    vocabulary, k=1.0, start_with=None):\n",
        "    model_counts = len(n_gram_counts_list)\n",
        "    suggestions = []\n",
        "    for i in range(model_counts-1):\n",
        "        n_gram_counts = n_gram_counts_list[i]\n",
        "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
        "\n",
        "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
        "                                    n_plus1_gram_counts, vocabulary,\n",
        "                                    k=k, start_with=start_with)\n",
        "        suggestions.append(suggestion)\n",
        "    return suggestions"
      ],
      "metadata": {
        "id": "znyWdON91FU-"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "Evaluate the model using perplexity."
      ],
      "metadata": {
        "id": "v68NSw9BSl2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(sentence, n_gram_counts,\n",
        "                         n_plus1_gram_counts, vocabulary_size,\n",
        "                         start_token=\"<s>\", end_token=\"<e>\", k=1.0):\n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "    sentence = [start_token] * n + sentence + [end_token]\n",
        "    sentence = tuple(sentence)\n",
        "    N = len(sentence)\n",
        "    product_pi = 1.0\n",
        "    for t in range(n, N):\n",
        "        ngram = sentence[t-n:t]\n",
        "        word = sentence[t]\n",
        "        probability = estimate_probability(word, ngram, n_gram_counts,\n",
        "                                           n_plus1_gram_counts,\n",
        "                                           vocabulary_size, k)\n",
        "        product_pi = product_pi * (1/probability)\n",
        "    perplexity = product_pi**(1/len(sentence))\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "wByeMDBistv2"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demostration"
      ],
      "metadata": {
        "id": "mZAHtDQpSnqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_gram_counts_list = []\n",
        "for n in range(1, 6):\n",
        "    n_model_counts = count_ngrams(train_data1, n)\n",
        "    n_gram_counts_list.append(n_model_counts)"
      ],
      "metadata": {
        "id": "ZU-dsIIZ6Ly6"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a sentence in lower case"
      ],
      "metadata": {
        "id": "yaU8jQMl2Xju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOf-bxME6bFM",
        "outputId": "0f7be89d-1fb5-4917-d0e2-889ccfe2d1bb"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14585"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"i love to watch\""
      ],
      "metadata": {
        "id": "vzZk2CRG6E9J"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "previous_tokens = sentence.split()\n",
        "tmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocab, k=1.0)\n",
        "\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(tmp_suggest4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "hz3MiUi013Vr",
        "outputId": "3512c599-6041-4acf-b252-a731839f3e23"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The previous words are ['i', 'love', 'to', 'watch'], the suggestions are:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('i', 6.855419208884623e-05),\n",
              " ('i', 6.855419208884623e-05),\n",
              " ('i', 6.855419208884623e-05),\n",
              " ('<e>', 0.0010968670734215397)]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            Based on\n",
        "        </td>\n",
        "        <td>\n",
        "            Assignments from the Natural Language Processing Specialization\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "eAf_NVYS6djz"
      }
    }
  ]
}