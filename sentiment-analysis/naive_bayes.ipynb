{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDfNZISIvrLdp3p/1RYsif",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thedatadj/natural-language-processing/blob/main/sentiment-analysis/naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            <b>Model</b>\n",
        "        </td>\n",
        "        <td>\n",
        "            Naive Bayes\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            <b>Task</b>\n",
        "        </td>\n",
        "        <td>\n",
        "            Classify a tweet as having a positive sentiment or a negative sentiment.\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            <b>Main library</b>\n",
        "        </td>\n",
        "        <td>\n",
        "            NLTK\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            <b>Dataset</b>\n",
        "        </td>\n",
        "        <td>\n",
        "            twitter_samples from NLTK datasets.\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            Based on\n",
        "        </td>\n",
        "        <td>\n",
        "             An assignment from the Natural Language Processing Specialization in coursera.\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "T93JIcsA2si8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "vwuKiyhL23ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "BdBBsM5cqX7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Twitter dataset\n",
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "nltk.download('twitter_samples')"
      ],
      "metadata": {
        "id": "A6WQeZ5q8FAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff225ae2-925b-4d86-b384-069f379b5e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the sets of positive and negative tweets."
      ],
      "metadata": {
        "id": "CDAVd0m8beRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "all_positive_tweets[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABPmEw5gbYLM",
        "outputId": "6da10e60-21f0-4be5-a79a-c920c2a1fba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
              " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
              " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the sets into a training set, and a testing set."
      ],
      "metadata": {
        "id": "bdMRz7mlbici"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg"
      ],
      "metadata": {
        "id": "tVkvnG6JqPjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set of labels for the tweets, independent of the number of tweets."
      ],
      "metadata": {
        "id": "QjnmEwJ6buD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
      ],
      "metadata": {
        "id": "8SsFuyCpbtd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "117djcF-qjPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# String manipulation\n",
        "import re"
      ],
      "metadata": {
        "id": "LttWwxDrsXWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove unnecessary characters from the tweets.**"
      ],
      "metadata": {
        "id": "-4sQhGm0uLkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x0 = []\n",
        "for tweet in train_x:\n",
        "    tweet = re.sub(r\"\\$\\w*\", \"\", tweet)\n",
        "    tweet = re.sub(r\"^RT[\\s]+\", \"\", tweet)\n",
        "    tweet = re.sub(r\"https?://[^\\s\\n\\r]+\", \"\", tweet)\n",
        "    tweet = re.sub(r\"#\", \"\", tweet)\n",
        "    train_x0.append(tweet)\n",
        "train_x = train_x0"
      ],
      "metadata": {
        "id": "hUHd89tHr7ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization, stemming, remove punctuations and stopwords.**\n"
      ],
      "metadata": {
        "id": "EpCJcbu2urLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "i6QvN9Onuszf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TweetTokenizer(preserve_case=False,\n",
        "                           strip_handles=True,\n",
        "                           reduce_len=True)"
      ],
      "metadata": {
        "id": "pYel2sG2vl6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "xxJrg7HN7sh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "DXbVHNhs8bkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stopwords_list = stopwords.words(\"english\")\n",
        "train_x0 = []\n",
        "for tweet in train_x:\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "    tweet_clean = []\n",
        "    for token in tweet_tokens:\n",
        "        if token not in stopwords_list and token not in string.punctuation:\n",
        "            token_stem = stemmer.stem(token)\n",
        "            tweet_clean.append(token_stem)\n",
        "    train_x0.append(tweet_clean)\n",
        "train_x = train_x0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTYETMED6kjG",
        "outputId": "2fce104d-d2d4-4a43-8a88-bf5d3a32b2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of a tweet after preprocessing."
      ],
      "metadata": {
        "id": "n618prqvfP78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNPnUDn4fTU8",
        "outputId": "496db371-5d3a-4e49-9aa4-cfe3617b74a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dictionary of frequencies\n"
      ],
      "metadata": {
        "id": "HAVoQF7fcx24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join the tweet and its label into a tuple."
      ],
      "metadata": {
        "id": "aC8gDjQFfbPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set of tweets and its label\n",
        "data = list(zip(train_x, train_y))\n",
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgtfGx_yejuj",
        "outputId": "9694a24d-b559-4185-e7c3-9df62ac9f7e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)'], 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a dictionary mapping (word, class) to its frequency."
      ],
      "metadata": {
        "id": "2dRxxaAOgoZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dicti = {}\n",
        "for tweet, label in data:\n",
        "    for token in tweet:\n",
        "        if (token, label) not in dicti:\n",
        "            dicti[(token, label)] = 0\n",
        "        dicti[(token, label)] += 1"
      ],
      "metadata": {
        "id": "gbPRoVHdfnpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dicti[('top', 1)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6x-3DvejTK9",
        "outputId": "e611b705-c7fb-41a5-e5de-f7af2ed9ed31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "2hCbWq_Vi7j8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I need to create a probability for each word in the vocabulary `vocab` being part of each class.\n",
        "\n",
        "$P(hi|1) = \\frac{\\text{Number of times that word appears in the positive class}}{\\text{Number of total words in the positive class}}$"
      ],
      "metadata": {
        "id": "4qrlUjvojHBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(set([t[0] for t in dicti.keys()]))\n",
        "vocab[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xek3u9rnqqwe",
        "outputId": "aa970823-c235-4adb-a315-cbc748ffd4c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['st',\n",
              " 'beeti',\n",
              " 'üçπ',\n",
              " 'owli',\n",
              " 'edward',\n",
              " 'tropic',\n",
              " 'reaali',\n",
              " 'supernatur',\n",
              " 'poorli',\n",
              " 'tard']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nv = len(vocab)\n",
        "nv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy_GqFe0q412",
        "outputId": "ca4bfff8-5f1b-473d-e469-3db0268ee1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9161"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute:\n",
        "* Total number of positive words\n",
        "* Total number of negative words"
      ],
      "metadata": {
        "id": "c-7AU9p0n1FF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Total number of positive words\n",
        "totalpos = 0\n",
        "\n",
        "# Total number of negative words\n",
        "totalneg = 0\n",
        "\n",
        "for pair in dicti:\n",
        "    label = pair[1]\n",
        "    if label == 1:\n",
        "        frequency = dicti[pair]\n",
        "        totalpos += frequency\n",
        "    else:\n",
        "        frequency = dicti[pair]\n",
        "        totalneg += frequency"
      ],
      "metadata": {
        "id": "bTWuJjoWnWRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of positive words =\", totalpos)\n",
        "print(\"Total number of negative words =\", totalneg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Crc7MuSbnpfK",
        "outputId": "0ae782c2-4aad-46d5-a786-35561bde70d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of positive words = 27543\n",
            "Total number of negative words = 27137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I need to calculate the probability of each word."
      ],
      "metadata": {
        "id": "xjkskgNKqB8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = {}\n",
        "for word in vocab:\n",
        "    posfrequency = dicti.get((word, 1), 0)\n",
        "    negfrequency = dicti.get((word, 0), 0)\n",
        "    posprobability = (posfrequency + 1)/(totalpos + nv)\n",
        "    negprobability = (negfrequency + 1)/(totalneg + nv)\n",
        "    prediction[word] = np.log(posprobability/negprobability)"
      ],
      "metadata": {
        "id": "py-5GYL9qBOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qf_kwSeskM0",
        "outputId": "4cd50f4c-50f7-47b5-f4d0-4d0428dd9086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9161"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction\n",
        "A function that takes a tweet and predicts whether it has a positive sentiment or a negative sentiment."
      ],
      "metadata": {
        "id": "NlTE_eTksqNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(tweet):\n",
        "    # Tokenize tweet\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "    tweet_clean = []\n",
        "    for token in tweet_tokens:\n",
        "        if token not in stopwords_list and token not in string.punctuation:\n",
        "            token_stem = stemmer.stem(token)\n",
        "            tweet_clean.append(token_stem)\n",
        "\n",
        "    p = 0\n",
        "    for token in tweet_clean:\n",
        "        if token in prediction:\n",
        "            p += prediction[token]\n",
        "\n",
        "    return p"
      ],
      "metadata": {
        "id": "qBn6TiJcsrLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(\"She smiled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQS8fvoNtUiL",
        "outputId": "3fea0aa8-65b2-4f69-a357-d7788f1127c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.557492820301094"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(\"He laughed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GcCyXmatWaO",
        "outputId": "732b1f6d-2d8b-49a2-fc76-09494b6de2d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.1652737774400095"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    }
  ]
}